{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameBoard:\n",
    "    def __init__(self, board_size=4):\n",
    "        self.board_size = board_size\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.board_size, self.board_size))\n",
    "        self.current_player = 1  # 1 for X, -1 for O\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        return str(self.board.tolist())\n",
    "    \n",
    "    def is_valid_move(self, row, col):\n",
    "        return 0 <= row < self.board_size and 0 <= col < self.board_size and self.board[row][col] == 0\n",
    "    \n",
    "    def get_valid_moves(self):\n",
    "        return [(i, j) for i in range(self.board_size) for j in range(self.board_size) if self.board[i][j] == 0]\n",
    "    \n",
    "    def make_move(self, row, col):\n",
    "        if not self.is_valid_move(row, col):\n",
    "            return None, -10, True  # Invalid move penalty\n",
    "        \n",
    "        self.board[row][col] = self.current_player\n",
    "        \n",
    "        # Check win condition\n",
    "        if self.check_win():\n",
    "            return self.get_state(), 1, True\n",
    "        \n",
    "        # Check draw\n",
    "        if len(self.get_valid_moves()) == 0:\n",
    "            return self.get_state(), 0, True\n",
    "        \n",
    "        self.current_player *= -1  # Switch player\n",
    "        return self.get_state(), 0, False\n",
    "    \n",
    "    def check_win(self):\n",
    "        # Check rows, columns and diagonals\n",
    "        for i in range(self.board_size):\n",
    "            if abs(sum(self.board[i])) == self.board_size:  # Check rows\n",
    "                return True\n",
    "            if abs(sum(self.board[:, i])) == self.board_size:  # Check columns\n",
    "                return True\n",
    "        \n",
    "        # Check diagonals\n",
    "        if abs(sum([self.board[i][i] for i in range(self.board_size)])) == self.board_size:\n",
    "            return True\n",
    "        if abs(sum([self.board[i][self.board_size-1-i] for i in range(self.board_size)])) == self.board_size:\n",
    "            return True\n",
    "        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIteration:\n",
    "    def __init__(self, env, gamma=0.9):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.values = defaultdict(float)\n",
    "        self.policy = {}\n",
    "    \n",
    "    def train(self, num_iterations=1000):\n",
    "        for _ in range(num_iterations):\n",
    "            delta = 0\n",
    "            states_to_update = set()\n",
    "            \n",
    "            # Generate states to update\n",
    "            self.env.reset()\n",
    "            self._generate_states(self.env.get_state(), states_to_update)\n",
    "            \n",
    "            # Update values\n",
    "            for state in states_to_update:\n",
    "                board = eval(state)\n",
    "                self.env.board = np.array(board)\n",
    "                old_value = self.values[state]\n",
    "                self.values[state] = self._get_max_value(state)\n",
    "                delta = max(delta, abs(old_value - self.values[state]))\n",
    "    \n",
    "    def _generate_states(self, state, states):\n",
    "        states.add(state)\n",
    "        board = eval(state)\n",
    "        self.env.board = np.array(board)\n",
    "        \n",
    "        if self.env.check_win() or len(self.env.get_valid_moves()) == 0:\n",
    "            return\n",
    "        \n",
    "        for move in self.env.get_valid_moves():\n",
    "            self.env.board = np.array(board)  # Reset board\n",
    "            next_state, _, _ = self.env.make_move(move[0], move[1])\n",
    "            if next_state not in states:\n",
    "                self._generate_states(next_state, states)\n",
    "    \n",
    "    def _get_max_value(self, state):\n",
    "        board = eval(state)\n",
    "        self.env.board = np.array(board)\n",
    "        \n",
    "        if self.env.check_win():\n",
    "            return 1 if self.env.current_player == 1 else -1\n",
    "        \n",
    "        valid_moves = self.env.get_valid_moves()\n",
    "        if not valid_moves:\n",
    "            return 0\n",
    "        \n",
    "        values = []\n",
    "        for move in valid_moves:\n",
    "            self.env.board = np.array(board)  # Reset board\n",
    "            next_state, reward, done = self.env.make_move(move[0], move[1])\n",
    "            if done:\n",
    "                values.append(reward)\n",
    "            else:\n",
    "                values.append(reward + self.gamma * -self._get_max_value(next_state))  # Negative because opponent's turn\n",
    "        \n",
    "        return max(values)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        board = eval(state)\n",
    "        self.env.board = np.array(board)\n",
    "        valid_moves = self.env.get_valid_moves()\n",
    "        \n",
    "        if not valid_moves:\n",
    "            return None\n",
    "        \n",
    "        best_value = float('-inf')\n",
    "        best_move = None\n",
    "        \n",
    "        for move in valid_moves:\n",
    "            self.env.board = np.array(board)  # Reset board\n",
    "            next_state, reward, done = self.env.make_move(move[0], move[1])\n",
    "            if done:\n",
    "                value = reward\n",
    "            else:\n",
    "                value = reward + self.gamma * -self.values[next_state]  # Negative because opponent's turn\n",
    "            \n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_move = move\n",
    "        \n",
    "        return best_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, agent, opponent_policy='random'):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if env.current_player == 1:  # Agent's turn\n",
    "            action = agent.get_action(state)\n",
    "        else:  # Opponent's turn\n",
    "            if opponent_policy == 'random':\n",
    "                valid_moves = env.get_valid_moves()\n",
    "                action = random.choice(valid_moves) if valid_moves else None\n",
    "        \n",
    "        if action is None:\n",
    "            break\n",
    "        \n",
    "        state, reward, done = env.make_move(action[0], action[1])\n",
    "    \n",
    "    if env.check_win():\n",
    "        return 1 if env.current_player == -1 else -1  # Return 1 if agent wins, -1 if opponent wins\n",
    "    return 0  # Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, agent, num_games=100):\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "    \n",
    "    for _ in range(num_games):\n",
    "        result = play_game(env, agent)\n",
    "        if result == 1:\n",
    "            wins += 1\n",
    "        elif result == -1:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "    \n",
    "    return wins / num_games, losses / num_games, draws / num_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation for both board sizes\n",
    "board_sizes = [4, 5]\n",
    "results = {}\n",
    "\n",
    "for size in board_sizes:\n",
    "    env = GameBoard(board_size=size)\n",
    "    agent = ValueIteration(env)\n",
    "    agent.train(num_iterations=1000)\n",
    "    \n",
    "    win_rates = []\n",
    "    eval_intervals = list(range(0, 1001, 100))\n",
    "    \n",
    "    for i in eval_intervals:\n",
    "        if i > 0:\n",
    "            agent.train(num_iterations=100)\n",
    "        win_rate, loss_rate, draw_rate = evaluate_agent(env, agent)\n",
    "        win_rates.append(win_rate)\n",
    "    \n",
    "    results[size] = (eval_intervals, win_rates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "for size in board_sizes:\n",
    "    eval_intervals, win_rates = results[size]\n",
    "    plt.plot(eval_intervals, win_rates, label=f'{size}x{size} board')\n",
    "\n",
    "plt.xlabel('Training Iterations')\n",
    "plt.ylabel('Win Rate')\n",
    "plt.title('RL Agent Win Rate vs Training Iterations')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Display final win rates\n",
    "for size in board_sizes:\n",
    "    _, win_rates = results[size]\n",
    "    print(f\"Final win rate for {size}x{size} board: {win_rates[-1]:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
